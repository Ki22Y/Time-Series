{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T16:50:39.576063Z",
     "iopub.status.busy": "2025-01-21T16:50:39.575636Z",
     "iopub.status.idle": "2025-01-21T16:50:39.58091Z",
     "shell.execute_reply": "2025-01-21T16:50:39.579908Z",
     "shell.execute_reply.started": "2025-01-21T16:50:39.576029Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T16:45:23.195243Z",
     "iopub.status.busy": "2025-01-21T16:45:23.194618Z",
     "iopub.status.idle": "2025-01-21T16:45:25.048072Z",
     "shell.execute_reply": "2025-01-21T16:45:25.047366Z",
     "shell.execute_reply.started": "2025-01-21T16:45:23.195211Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/train.csv')\n",
    "test_data = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/test.csv')\n",
    "oil_data = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/oil.csv')\n",
    "transactions_data = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/transactions.csv')\n",
    "holidays_data = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv')\n",
    "stores_data = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/stores.csv')\n",
    "\n",
    "# Combine train_data and test_data.\n",
    "combined_data = pd.concat([train_data, test_data], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T16:45:25.049316Z",
     "iopub.status.busy": "2025-01-21T16:45:25.049068Z",
     "iopub.status.idle": "2025-01-21T16:45:26.672006Z",
     "shell.execute_reply": "2025-01-21T16:45:26.671096Z",
     "shell.execute_reply.started": "2025-01-21T16:45:25.049297Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to summarize the data frames \n",
    "def resumetable(df):\n",
    "    summary = pd.DataFrame(df.dtypes, columns=['Data Type'])\n",
    "    summary['Missing Values'] = df.isna().sum().values\n",
    "    summary['Unique Values'] = df.nunique().values\n",
    "    summary['Min Value'] = df.min().values\n",
    "    summary['Max Value'] = df.max().values\n",
    "    return summary\n",
    "\n",
    "def print_with_title(title, content):\n",
    "    print(\"=\" * 30)\n",
    "    print(title)\n",
    "    print(\"=\" * 30)\n",
    "    print(content)\n",
    "\n",
    "train_summary = resumetable(train_data)\n",
    "print_with_title(\"train_data summary\", train_summary)\n",
    "\n",
    "test_summary = resumetable(test_data)\n",
    "print_with_title(\"test_data summary\", test_summary)\n",
    "\n",
    "holiday_summary = resumetable(holidays_data)\n",
    "print_with_title(\"holidays_data summary\", holiday_summary)\n",
    "\n",
    "oil_summary = resumetable(oil_data)\n",
    "print_with_title(\"oil_data summary\", oil_summary)\n",
    "\n",
    "transactions_summary = resumetable(transactions_data)\n",
    "print_with_title(\"transactions_data summary\", transactions_summary)\n",
    "\n",
    "stores_summary = resumetable(stores_data)\n",
    "print_with_title(\"stores_data summary\", stores_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T16:45:26.673661Z",
     "iopub.status.busy": "2025-01-21T16:45:26.673297Z",
     "iopub.status.idle": "2025-01-21T16:45:30.485671Z",
     "shell.execute_reply": "2025-01-21T16:45:30.484731Z",
     "shell.execute_reply.started": "2025-01-21T16:45:26.673631Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_data['date'] = pd.to_datetime(combined_data['date'])\n",
    "\n",
    "# Create new features from date \n",
    "combined_data['year'] = combined_data['date'].dt.year  \n",
    "combined_data['month'] = combined_data['date'].dt.month  \n",
    "combined_data['day'] = combined_data['date'].dt.day\n",
    "combined_data['day_of_week'] = combined_data['date'].dt.dayofweek\n",
    "combined_data['is_weekend'] = combined_data['date'].dt.dayofweek.apply(lambda x: 1 if x >= 5 else 0)\n",
    "combined_data['is_payday'] = combined_data['date'].dt.day.apply(lambda x: 1 if x in [15, 30] else 0)\n",
    "\n",
    "combined_summary = resumetable(combined_data)\n",
    "print_with_title(\"combined_data summary\", combined_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T16:45:30.486894Z",
     "iopub.status.busy": "2025-01-21T16:45:30.486614Z",
     "iopub.status.idle": "2025-01-21T16:45:31.219536Z",
     "shell.execute_reply": "2025-01-21T16:45:31.218585Z",
     "shell.execute_reply.started": "2025-01-21T16:45:30.486873Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function for payday variable \n",
    "def add_payday_flow(data):\n",
    "    data['payday_flow'] = data['date'].dt.day.apply(\n",
    "        lambda x: '30_before' if x >= 24 or x <= 1 else\n",
    "                  '30_after' if 2 <= x <= 8 else\n",
    "                  '15_before' if 9 <= x <= 14 else\n",
    "                  '15_after' if 16 <= x <= 23 else 'other'\n",
    "    )\n",
    "    return data\n",
    "\n",
    "combined_data = add_payday_flow(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T16:45:31.222205Z",
     "iopub.status.busy": "2025-01-21T16:45:31.221936Z",
     "iopub.status.idle": "2025-01-21T16:45:31.249606Z",
     "shell.execute_reply": "2025-01-21T16:45:31.248774Z",
     "shell.execute_reply.started": "2025-01-21T16:45:31.222185Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to process oil-related information. \n",
    "def preprocess_oil_data(oil_data):\n",
    "    oil_data['date'] = pd.to_datetime(oil_data['date'])\n",
    "    oil_data.set_index('date', inplace=True)\n",
    "    oil_data['dcoilwtico'] = oil_data['dcoilwtico'].interpolate(method='time').bfill()\n",
    "    oil_data.reset_index(inplace=True) \n",
    "    return oil_data.rename(columns={'dcoilwtico': 'oil_price'})\n",
    "\n",
    "oil = preprocess_oil_data(oil_data)\n",
    "print_with_title(\"Summary of oil_data\", oil)\n",
    "\n",
    "# Function to incorporate store information.\n",
    "def preprocess_store_data(stores_data):\n",
    "    stores_data.rename(columns={\n",
    "        'city': 'store_city',\n",
    "        'state': 'store_state',\n",
    "        'type': 'store_type',\n",
    "    }, inplace=True)\n",
    "    return stores_data\n",
    "\n",
    "stores = preprocess_store_data(stores_data)\n",
    "print_with_title(\"Summary of stores_data stores_data\", stores)\n",
    "\n",
    "# Function to incorporate transaction information.\n",
    "def preprocess_transactions_data(transactions_data):\n",
    "    transactions_data['date'] = pd.to_datetime(transactions_data['date'])\n",
    "    return transactions_data\n",
    "\n",
    "transactions = preprocess_transactions_data(transactions_data)\n",
    "print_with_title(\"Summary of transactions_data\", transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T16:45:31.250657Z",
     "iopub.status.busy": "2025-01-21T16:45:31.250446Z",
     "iopub.status.idle": "2025-01-21T16:45:32.687048Z",
     "shell.execute_reply": "2025-01-21T16:45:32.686062Z",
     "shell.execute_reply.started": "2025-01-21T16:45:31.25064Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_features(combined_data, oil, transactions, stores):\n",
    "  \n",
    "    # Combine oil prices\n",
    "    combined_data = combined_data.merge(oil, on='date', how='left')\n",
    "    combined_data['oil_price'] = combined_data['oil_price'].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "    # Combine transactions\n",
    "    combined_data = combined_data.merge(transactions, on=['date', 'store_nbr'], how='left')\n",
    "    combined_data['transactions'] = combined_data['transactions'].fillna(0)\n",
    "\n",
    "    # Combine store characteristics \n",
    "    store_columns = ['store_nbr', 'store_city', 'store_state', 'store_type', 'cluster']\n",
    "    combined_data = combined_data.merge(stores[store_columns], on='store_nbr', how='left')\n",
    "\n",
    "    return combined_data\n",
    "    \n",
    "combined_data = merge_features(combined_data, oil, transactions, stores)\n",
    "print(combined_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T16:45:32.68868Z",
     "iopub.status.busy": "2025-01-21T16:45:32.688342Z",
     "iopub.status.idle": "2025-01-21T16:46:08.904401Z",
     "shell.execute_reply": "2025-01-21T16:46:08.903478Z",
     "shell.execute_reply.started": "2025-01-21T16:45:32.68865Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_holiday_data(holidays_data, combined_data):\n",
    "    holidays_data['date'] = pd.to_datetime(holidays_data['date'])\n",
    "    holiday_data_filtered = holidays_data[\n",
    "        (holidays_data['transferred'] != True) &  \n",
    "        (holidays_data['type'] != \"Work Day\")    \n",
    "    ].copy()\n",
    "\n",
    "    holiday_data_filtered['is_holiday'] = np.where(\n",
    "        holiday_data_filtered['type'] == \"Holiday\", \"Yes\", \"No\"\n",
    "    )\n",
    "\n",
    "    holiday_data_filtered['holiday_location'] = np.where(\n",
    "        holiday_data_filtered['type'] == \"Holiday\",\n",
    "        holiday_data_filtered['locale'],\n",
    "        \"Non-Holiday\"\n",
    "    )\n",
    "\n",
    "    holiday_data_filtered['holiday_location_name'] = np.where(\n",
    "        holiday_data_filtered['type'] == \"Holiday\",\n",
    "        holiday_data_filtered['locale_name'],\n",
    "        \"Non-Holiday\"\n",
    "    )\n",
    "\n",
    "    combined_data = combined_data.merge(\n",
    "        holiday_data_filtered[['date', 'is_holiday', 'holiday_location', 'holiday_location_name']],\n",
    "        on='date',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    combined_data['is_holiday'] = combined_data['is_holiday'].fillna(\"No\")\n",
    "    combined_data['holiday_location'] = combined_data['holiday_location'].fillna(\"Non-Holiday\")\n",
    "    combined_data['holiday_location_name'] = combined_data['holiday_location_name'].fillna(\"Non-Holiday\")\n",
    "\n",
    "    # Create a holiday indicator for the store's location.\n",
    "    def is_holiday_in_store_location(row):\n",
    "        if row['is_holiday'] == \"Yes\" and row['holiday_location'] == \"National\":\n",
    "            return \"Yes\"\n",
    "        elif row['is_holiday'] == \"Yes\" and row['holiday_location'] == \"Local\" and row['store_city'] == row['holiday_location_name']:\n",
    "            return \"Yes\"\n",
    "        elif row['is_holiday'] == \"Yes\" and row['holiday_location'] == \"Regional\" and row['store_state'] == row['holiday_location_name']:\n",
    "            return \"Yes\"\n",
    "        else:\n",
    "            return \"No\"\n",
    "\n",
    "    combined_data['is_holiday_in_store_location'] = combined_data.apply(is_holiday_in_store_location, axis=1)\n",
    "\n",
    "    combined_data['is_holiday_in_store_location'] = combined_data['is_holiday_in_store_location'].astype('category')\n",
    "\n",
    "    # Remove unnecessary columns.\n",
    "    columns_to_drop = ['date', 'holiday_location', 'holiday_location_name', 'type', 'description', 'transferred', 'locale', 'locale_name', 'is_holiday']\n",
    "    combined_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "combined_data = preprocess_holiday_data(holidays_data, combined_data)\n",
    "print(combined_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the accuracy of our predictions, we will scale some numerical columns, such as 'onpromotion' and 'oil_price'. The 'oil_price' column represents the price of oil on a specific day, while 'onpromotion' indicates the number of items on promotion on that day. These two metrics not only have different units but also vary in scale and range, which could impact the performance of the model.\n",
    "\n",
    "By scaling these features, we ensure that their ranges are comparable, preventing any single feature from disproportionately influencing the model due to its magnitude. o ensure there is no data leakage, we will apply the scaling separately for the train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T16:46:08.905834Z",
     "iopub.status.busy": "2025-01-21T16:46:08.905555Z",
     "iopub.status.idle": "2025-01-21T16:46:09.553289Z",
     "shell.execute_reply": "2025-01-21T16:46:09.552495Z",
     "shell.execute_reply.started": "2025-01-21T16:46:08.905797Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split into train and test considering the sales variable\n",
    "train = combined_data[~combined_data['sales'].isna()].copy()\n",
    "test = combined_data[combined_data['sales'].isna()].copy()\n",
    "\n",
    "# Select columns to scale\n",
    "columns_to_scale = ['onpromotion', 'oil_price']\n",
    "\n",
    "# Create the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale the values\n",
    "train[columns_to_scale] = scaler.fit_transform(train[columns_to_scale])\n",
    "test[columns_to_scale] = scaler.fit_transform(test[columns_to_scale])\n",
    "\n",
    "print(train.head())\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['year', 'month', 'day']\n",
    "\n",
    "for var in variables:\n",
    "    # Group by each unique value and calculate the average\n",
    "    data_grouped = train.groupby(var)['sales'].mean().reset_index()\n",
    "    \n",
    "    # Plot grouped data\n",
    "    plt.plot(data_grouped[var], data_grouped['sales'], marker='o')\n",
    "    plt.title(f'Relationship between {var} and sales')\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel('Average sales')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['store_city', 'store_state', 'family', 'store_nbr'] \n",
    "\n",
    "for var in variables: \n",
    "    # Group by the current variable and calculate the average sales\n",
    "    grouped_data = train.groupby(var)['sales'].mean().reset_index()\n",
    "\n",
    "    # Create a bar chart\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.barplot(data=grouped_data, x=var, y='sales')\n",
    "    plt.title(f'Promedio de ventas por {var}')\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel('Promedio de ventas')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T16:46:36.11612Z",
     "iopub.status.busy": "2025-01-21T16:46:36.115581Z",
     "iopub.status.idle": "2025-01-21T16:46:39.82773Z",
     "shell.execute_reply": "2025-01-21T16:46:39.826934Z",
     "shell.execute_reply.started": "2025-01-21T16:46:36.116084Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = ['is_holiday_in_store_location', 'store_city', 'store_state', 'store_type', 'cluster', 'payday_flow', 'family']\n",
    "\n",
    "def encode_categorical_columns(dataframe, columns):\n",
    "    for col in columns:\n",
    "        dataframe[col] = pd.Categorical(dataframe[col]).codes  \n",
    "    return dataframe\n",
    "\n",
    "train_v2 = encode_categorical_columns(train, columns)\n",
    "test_v2 = encode_categorical_columns(test, columns)\n",
    "\n",
    "correlaciones = train_v2.corr()  \n",
    "correlation_target = correlaciones['sales'].sort_values(ascending=False)\n",
    "print(correlation_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T16:48:26.744255Z",
     "iopub.status.busy": "2025-01-21T16:48:26.743933Z",
     "iopub.status.idle": "2025-01-21T16:48:27.76049Z",
     "shell.execute_reply": "2025-01-21T16:48:27.759665Z",
     "shell.execute_reply.started": "2025-01-21T16:48:26.744235Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combine train_data and test_data\n",
    "combined_data_v2 = pd.concat([train_v2, test_v2], axis=0, ignore_index=True)\n",
    "\n",
    "def calculate_family_averages(data):\n",
    "    averages = data.groupby('family')[['sales', 'transactions']].mean().reset_index()\n",
    "    averages.rename(columns={'sales': 'avg_sales', 'transactions': 'avg_transactions'}, inplace=True)\n",
    "    return averages\n",
    "\n",
    "# Calculate averages by family and add the new columns to the original DataFrame\n",
    "family_averages = calculate_family_averages(combined_data_v2)\n",
    "combined_data_v2 = combined_data_v2.merge(family_averages, on='family', how='left')\n",
    "\n",
    "# Remove the 'transactions' column if it is no longer needed\n",
    "combined_data_v2.drop(columns=['transactions'], inplace=True, errors='ignore')\n",
    "\n",
    "train_v3 = combined_data_v2[~combined_data_v2['sales'].isna()].copy()\n",
    "test_v3 = combined_data_v2[combined_data_v2['sales'].isna()].copy()\n",
    "\n",
    "print(train_v3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T16:50:45.708382Z",
     "iopub.status.busy": "2025-01-21T16:50:45.707689Z",
     "iopub.status.idle": "2025-01-21T17:58:14.739882Z",
     "shell.execute_reply": "2025-01-21T17:58:14.739046Z",
     "shell.execute_reply.started": "2025-01-21T16:50:45.708353Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transform sales to logarithmic scale\n",
    "train_v3['sales_log'] = np.log1p(train_v3['sales'])  \n",
    "\n",
    "# Separate independent and dependent variables\n",
    "X = train_v3.drop(columns=['sales', 'sales_log'])  \n",
    "y = train_v3['sales_log']  \n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameter selection for CatBoost\n",
    "catboost_model = CatBoostRegressor(\n",
    "    eval_metric='RMSE',\n",
    "    verbose=False,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "param_distributions = {\n",
    "    'iterations': [500, 1000, 2000],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'depth': [4, 6, 8, 10],\n",
    "    'l2_leaf_reg': [1, 3, 5, 7],\n",
    "    'bagging_temperature': [0.2, 0.5, 0.7, 1.0]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=catboost_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=10,                   \n",
    "    scoring='neg_mean_squared_error',  \n",
    "    cv=3,                        \n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best hyperparameters:\", random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T20:24:30.886761Z",
     "iopub.status.busy": "2025-01-21T20:24:30.886109Z",
     "iopub.status.idle": "2025-01-21T20:27:42.753625Z",
     "shell.execute_reply": "2025-01-21T20:27:42.752734Z",
     "shell.execute_reply.started": "2025-01-21T20:24:30.886732Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train a CatBoost model with the previously obtained hyperparameters\n",
    "catboost_model = CatBoostRegressor(\n",
    "    iterations=1000,           \n",
    "    learning_rate=0.1,        \n",
    "    depth=8,                   \n",
    "    eval_metric='RMSE',        \n",
    "    verbose=False,             \n",
    "    random_seed=42,\n",
    "    l2_leaf_reg=5,\n",
    "    bagging_temperature=0.7\n",
    ")\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the logarithmic scale and transform them back to the original scale\n",
    "catboost_predictions_log = catboost_model.predict(X_test)\n",
    "catboost_predictions_original = np.expm1(catboost_predictions_log)  \n",
    "\n",
    "# Transform y_test back to the original scale\n",
    "y_test_original = np.expm1(y_test)\n",
    "\n",
    "# Define the RMSLE metric and evaluate the model on the original scale\n",
    "def rmsle(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square(np.log1p(y_pred) - np.log1p(y_true))))\n",
    "\n",
    "mae = mean_absolute_error(y_test_original, catboost_predictions_original)\n",
    "mse = mean_squared_error(y_test_original, catboost_predictions_original)\n",
    "rmse = np.sqrt(mse)  \n",
    "r2 = r2_score(y_test_original, catboost_predictions_original)\n",
    "rmsle_value = rmsle(y_test_original, catboost_predictions_original)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"RMSLE: {rmsle_value:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "\n",
    "# Compare the first rows of predictions with actual values\n",
    "output_df = pd.DataFrame({\n",
    "    'Actual Sales (Original)': y_test_original,\n",
    "    'Predicted Sales (Original)': catboost_predictions_original\n",
    "})\n",
    "print(\"\\nFirst predictions of the model:\")\n",
    "print(output_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T18:37:29.745734Z",
     "iopub.status.busy": "2025-01-21T18:37:29.745413Z",
     "iopub.status.idle": "2025-01-21T18:37:29.771989Z",
     "shell.execute_reply": "2025-01-21T18:37:29.771235Z",
     "shell.execute_reply.started": "2025-01-21T18:37:29.74571Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "test_predictions_log = catboost_model.predict(test_v3) \n",
    "\n",
    "# Convert predictions back to the original scale\n",
    "test_predictions = np.expm1(test_predictions_log)\n",
    "\n",
    "# Create a DataFrame with the predictions and display the first rows \n",
    "output_catboost = pd.DataFrame({\n",
    "    'sales': test_predictions\n",
    "})\n",
    "print(output_catboost.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T18:37:35.364944Z",
     "iopub.status.busy": "2025-01-21T18:37:35.364251Z",
     "iopub.status.idle": "2025-01-21T19:32:33.364549Z",
     "shell.execute_reply": "2025-01-21T19:32:33.363623Z",
     "shell.execute_reply.started": "2025-01-21T18:37:35.364919Z"
    }
   },
   "outputs": [],
   "source": [
    "# Base model of LightGBM\n",
    "lightgbm_model = LGBMRegressor(\n",
    "    random_state=42,\n",
    "    verbose=-1  \n",
    ")\n",
    "\n",
    "# Hyperparameter search space\n",
    "param_distributions = {\n",
    "    'n_estimators': [500, 1000, 2000],  \n",
    "    'learning_rate': [0.01, 0.05, 0.1],  \n",
    "    'max_depth': [4, 6, 8, 10],  \n",
    "    'num_leaves': [20, 31, 50, 100],  \n",
    "    'min_child_samples': [10, 20, 30],  \n",
    "    'reg_alpha': [0, 0.1, 1, 10],  \n",
    "    'reg_lambda': [0, 0.1, 1, 10],  \n",
    "    'subsample': [0.6, 0.8, 1.0],  \n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]  \n",
    "}\n",
    "\n",
    "# RandomizedSearchCV for LightGBM\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lightgbm_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=10,  \n",
    "    scoring='neg_mean_squared_error',  \n",
    "    cv=3,  \n",
    "    random_state=42,\n",
    "    verbose=1  \n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best hyperparameters:\", random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T19:34:57.8023Z",
     "iopub.status.busy": "2025-01-21T19:34:57.80196Z",
     "iopub.status.idle": "2025-01-21T19:37:31.175477Z",
     "shell.execute_reply": "2025-01-21T19:37:31.174585Z",
     "shell.execute_reply.started": "2025-01-21T19:34:57.802276Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train a LightGBM model with the previously obtained hyperparameters\n",
    "lightgbm_model = LGBMRegressor(\n",
    "    n_estimators=1000,         \n",
    "    learning_rate=0.05,        \n",
    "    max_depth=8,               \n",
    "    random_state=42,\n",
    "    subsample=0.8, \n",
    "    reg_lambda=0.1, \n",
    "    reg_alpha=0, \n",
    "    num_leaves=100, \n",
    "    min_child_samples=20, \n",
    "    colsample_bytree=0.6,\n",
    "    verbose=-1    \n",
    ")\n",
    "lightgbm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the logarithmic scale and transform them back to the original scale\n",
    "lightgbm_predictions_log = lightgbm_model.predict(X_test)\n",
    "lightgbm_predictions_original = np.expm1(lightgbm_predictions_log)\n",
    "\n",
    "# Transform y_test back to the original scale\n",
    "y_test_original = np.expm1(y_test)\n",
    "\n",
    "# Define the RMSLE metric and evaluate the model on the original scale\n",
    "def rmsle(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square(np.log1p(y_pred) - np.log1p(y_true))))\n",
    "\n",
    "mae = mean_absolute_error(y_test_original, lightgbm_predictions_original)\n",
    "mse = mean_squared_error(y_test_original, lightgbm_predictions_original)\n",
    "rmse = np.sqrt(mse)  \n",
    "r2 = r2_score(y_test_original, lightgbm_predictions_original)\n",
    "rmsle_value = rmsle(y_test_original, lightgbm_predictions_original)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"RMSLE: {rmsle_value:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "\n",
    "# Compare the first rows of predictions with actual values\n",
    "output_df = pd.DataFrame({\n",
    "    'Actual Sales (Original)': y_test_original,\n",
    "    'Predicted Sales (Original)': lightgbm_predictions_original\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T19:38:47.877043Z",
     "iopub.status.busy": "2025-01-21T19:38:47.876393Z",
     "iopub.status.idle": "2025-01-21T19:38:49.78175Z",
     "shell.execute_reply": "2025-01-21T19:38:49.780937Z",
     "shell.execute_reply.started": "2025-01-21T19:38:47.877015Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test_new = test_v3[X_train.columns]\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions_log = lightgbm_model.predict(X_test_new) \n",
    "\n",
    "# Convert predictions back to the original scale\n",
    "test_predictions = np.expm1(test_predictions_log)\n",
    "\n",
    "# Create a DataFrame with the predictions and display the first rows\n",
    "output_lightgbm = pd.DataFrame({\n",
    "    'sales': test_predictions\n",
    "})\n",
    "print(output_lightgbm.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T19:39:17.949979Z",
     "iopub.status.busy": "2025-01-21T19:39:17.949662Z",
     "iopub.status.idle": "2025-01-21T19:39:17.956765Z",
     "shell.execute_reply": "2025-01-21T19:39:17.95585Z",
     "shell.execute_reply.started": "2025-01-21T19:39:17.949957Z"
    }
   },
   "outputs": [],
   "source": [
    "final_predictions = (output_catboost + output_lightgbm ) / 2\n",
    "\n",
    "print(final_predictions.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T19:39:35.721435Z",
     "iopub.status.busy": "2025-01-21T19:39:35.720781Z",
     "iopub.status.idle": "2025-01-21T19:39:35.785013Z",
     "shell.execute_reply": "2025-01-21T19:39:35.784269Z",
     "shell.execute_reply.started": "2025-01-21T19:39:35.721408Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the submission DataFrame with the columns 'id' and 'sales'\n",
    "submission = test[['id']].copy()  \n",
    "submission['sales'] = final_predictions['sales'].values  \n",
    "\n",
    "print(submission.head())\n",
    "\n",
    "# Save the CSV file\n",
    "submission.to_csv('/kaggle/working/final_submission.csv', index=False)\n",
    "print(\"Predictions file created.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 2887556,
     "sourceId": 29781,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
